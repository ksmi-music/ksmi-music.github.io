import { useState } from 'react';
import './Schedule.css';

// ì¬ì‚¬ìš© ê°€ëŠ¥í•œ PosterList ì»´í¬ë„ŒíŠ¸ ë§Œë“¤ê¸°
const PosterList = ({ professor, posters }) => {
  const [isOpen, setIsOpen] = useState(false);
  
  const togglePosters = (e) => {
    e.stopPropagation();
    setIsOpen(!isOpen);
  };
  
  return (
    <div className="collapsible-container">
      <p 
        className="collapsible-text"
        onClick={togglePosters}
      >
        {professor} í¬ìŠ¤í„° ëª©ë¡ {isOpen ? 'ë‹«ê¸° â–²' : 'ë³´ê¸° â–¼'}
      </p>
      <div className="poster-list" style={{ display: isOpen ? 'block' : 'none' }}>
        <ul>
          {posters.map((poster, index) => (
            <li key={index}>{poster}</li>
          ))}
        </ul>
      </div>
    </div>
  );
};

function Schedule() {
  // State for controlling modal visibility
  const [modalOpen, setModalOpen] = useState(false);
  const [modalContent, setModalContent] = useState({
    title: '',
    time: '',
    content: null
  });

  // Function to open modal with specific content
  const openModal = (title, time, content) => {
    setModalContent({
      title,
      time,
      content
    });
    setModalOpen(true);
  };

  // Function to close modal
  const closeModal = () => {
    setModalOpen(false);
  };

  return (
    <section className="schedule-section">
      <h2 style={{ textAlign: 'center' }}>ì‹¬í¬ì§€ì—„ ì¼ì •</h2>
      <p className="schedule-intro">ğŸµ ìŒì•…ê³¼ ê¸°ìˆ ì˜ ë§Œë‚¨! ğŸ¹ ì„¸ì…˜ì„ í´ë¦­í•˜ë©´ ìƒì„¸ ë‚´ìš©ì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.</p>
      
      <div className="compact-schedule">
        <div className="schedule-card" onClick={() => openModal('ê°œíšŒ', '10:00 - 10:30', 
          <>
            <p className="speaker">ì¸ì‚¬ë§ #1: Prof. Zhiyao Duan (University of Rochester, ISMIR president), <a className="link-badge" href='https://labsites.rochester.edu/air/index.html'>Audio Information Research (AIR) lab</a></p>
            
            <p><b>Bio</b></p>
            <p className="details">Zhiyao Duan received his BS in Automation and MS in Control Science and Engineering from Tsinghua University, China, in 2004 and 2008, respectively, and PhD in Computer Science from Northwestern University in 2013. He joined the faculty of the Electrical and Computer Engineering Department as an assistant professor in 2013, and is currently an associate professor in Electrical and Computer Engineering, Computer Science, and Data Science. His research interest is in computer audition and its connections with computer vision, natural language processing, and augmented and virtual reality. He received a best paper award at the SMC 2017, a best paper nomination at ISMIR 2017, and a CAREER award from the National Science Foundation (NSF). His research is funded by NSF, NIH, New York State Center of Excellence in Data Science, and University of Rochester internal awards on AR/VR, health analytics, and data science. He served as a Scientific Program Co-Chair of ISMIR 2021, and is serving as an associate editor for IEEE Open Journal of Signal Processing, a guest editor for Transactions of the International Society for Music Information Retrieval, and a guest editor for Frontiers in Signal Processing. He is the President-Elect of the International Society for Music Information Retrieval (ISMIR)</p>

            <p className="speaker">ì¸ì‚¬ë§ #2: Prof. Jinha Lee (University of Washington), <a className="link-badge" href='https://gamer.ischool.uw.edu/'>Game Research Group</a></p>
            
            <p><b>Bio</b></p>
            <p className="details">Jin Ha Lee is a Professor and Associate Dean for Faculty Affairs at the Information School in University of Washington and the director of the GAMER (GAME Research) Group. Her research interests include: music, game, and multimedia information seeking and retrieval, information organization and access, and knowledge representation. The GAMER Group explores new ideas and approaches for organizing and providing access to video games and interactive media, understanding user behavior related to popular cultural materials, and using these materials for informal learning for topics such as misinformation and mental health. She holds an M.S. (2002) and a Ph.D. (2008) from the University of Illinois at Urbana-Champaign.</p>

          </>
        )}>
          <div className="time-badge">10:00</div>
          <h3>ê°œíšŒ</h3>
          <div className="schedule-brief">Zhiyao Duan êµìˆ˜ ë° ì´ì§„í•˜ êµìˆ˜ ì¸ì‚¬ë§</div>
        </div>

        <div className="schedule-card keynote" onClick={() => openModal('Keynote', '10:30 - 11:30', 
          <>
            <div className="keynote-details">
              <h4>Advancing Music Experience Through Music Information Research</h4>
              <p className="speaker">Dr. Masataka Goto, Senior Principal Researcher, AIST </p> 

              <p><b>Abstract</b></p>
              <p className="details">Music technologies will open up new ways of enjoying music, both in terms of creating and appreciating music. In this keynote, I will discuss how music information research can enrich music experiences by introducing examples of our research outcomes. For example, "Lyric Apps" offer a new form of lyric-driven visual art, dynamically rendering different visual content based on user interaction. After releasing "TextAlive App API", a web-based framework for creating lyric apps, we have held annual programming contests since 2020. Another example is "Kiite Cafe", a web service that allows users to get together virtually to listen to music. It lets users enjoy the same song simultaneously while reacting in real time, creating a shared listening experience. In the future, further advances in music information research will make interactions between people and music more active and enriching.</p>
              
              <p><b>Bio</b></p>
              <p className="details">Masataka Goto received the Doctor of Engineering degree from Waseda University in 1998. He is currently the Senior Principal Researcher at the National Institute of Advanced Industrial Science and Technology (AIST), Japan. Over the past 33 years, he has published more than 350 papers in refereed journals and international conference proceedings and has received 72 awards, including several best paper awards, best presentation awards, the Tenth Japan Academy Medal, and the Tenth JSPS PRIZE. He has served as a committee member of over 140 scientific societies and conferences, including as the General Chair of ISMIR 2009 and 2014. From 2007, he chaired the Special Interest Group on Music and Computer (SIGMUS) under the Information Processing Society of Japan (IPSJ) for two years.</p>
            </div>
          </>
        )}>
          <div className="time-badge">10:30</div>
          <h3>Keynote</h3>
          <div className="schedule-brief">Dr. Masataka Goto, AIST</div>
        </div>

        <div className="schedule-card break">
          <div className="time-badge">11:30</div>
          <h3>ì ì‹¬ ì‹œê°„</h3>
          <div className="schedule-brief">ë„ì‹œë½ ì œê³µ</div>
        </div>

        <div className="schedule-card" onClick={() => openModal('ì—°êµ¬ì‹¤ ë° ê¸°ì—… ì†Œê°œ ì„¸ì…˜ #1', '12:30 - 13:30', 
          <>
            <p className="speaker"> MARG, ì„œìš¸ëŒ€ (ì´êµêµ¬ êµìˆ˜) <a className="link-badge" href='https://marg.snu.ac.kr/'>Homepage</a></p>
            <p> Music and Audio Research Group (MARG) at Seoul National University is a highly interdisciplinary research group that tries to address intriguing challenges in music and audio. With digital audio signal processing (DSP) and machine learning (ML) as two main tools, we have been tackling these problems: the best part is, as we learn more about human's auditory perception and cognition, the more interesting problems we discover.</p>
            <p className="speaker"> AIRIS Lab, KAIST (ê¹€ì„±ì˜ êµìˆ˜) <a className="link-badge" href='https://airislab.kaist.ac.kr/'>Homepage</a></p>
            <p>AIRIS Lab is home to a team of dedicated researchers and innovators passionate about exploring the boundaries of immersive audio and psychoacoustics. Our mission is to bridge the gap between technological advancements and human auditory perception, creating immersive and holistic sound experiences that resonate with users on a profound level.</p>
            <p className="speaker"> Maler Lab, ì„œê°•ëŒ€ (ì •ë‹¤ìƒ˜ êµìˆ˜) <a className="link-badge" href='https://malerlab.github.io/'>Homepage</a></p>
            <p>Music and Art Learning (MALer) Lab aims to understand music and art computationally, especially through deep learning. Our research interests covers broad music information retrieval including computational modeling of music generation and music performance, computational musicology, and cross-modal generation.</p>
            <p className="speaker"> AAA Lab, ê²½ë¶ëŒ€ (ì´ì„ì§„ êµìˆ˜) <a className="link-badge" href='https://sites.google.com/view/knuaaalab/home?authuser=0'>Homepage</a></p>
            <p>The researches of Applied Acoustics and Audio (AAA) laboratory are focused on signal processing issues related to generate, transmit, receive, analyze, and control the acoustic signals based on theories of acoustics and human hearing. Our research activities may cover the topics related to audio broadcasting, multichannel sound reproduction, acoustic source separation, music signal analysis, and even ultrasonic and sonar systems. We also welcome any interesting ideas related to acoustic phenomenon in real world.</p>
            <p className="speaker"> ê°€ìš°ë””ì˜¤ (TBA) <a className="link-badge" href='https://www.gaudiolab.com/ko/'>Homepage</a></p>
            <p>ê°€ìš°ë””ì˜¤ë©ì€ ì„¸ê³„ì ìœ¼ë¡œ ì†ê¼½íˆëŠ” ì˜¤ë””ì˜¤ ê¸°ìˆ ì„ í†µí•´, ì—¬ëŸ¬ë¶„ê»˜ì„œ ì „í˜€ ê²½í—˜í•´ ë³´ì§€ ëª»í–ˆë˜ ìƒˆë¡œìš´ ì°¨ì›ì˜ ì†Œë¦¬ ê²½í—˜ì„ ì œê³µí•˜ëŠ” íšŒì‚¬ì…ë‹ˆë‹¤. ë©”íƒ€ë²„ìŠ¤ì˜ ì™„ì„±ì„ ë‹´ë‹¹í•˜ëŠ” ìŠ¤í˜ì´ì…œ ì˜¤ë””ì˜¤(ê³µê°„ìŒí–¥)ì™€ AI ì˜¤ë””ì˜¤ ê¸°ìˆ ì˜ ê°€ì¥ ì²¨ë‹¨, ê·¸ ì´ìƒì˜ ê²½ì§€ì—ì„œ ë¹„êµ ë¶ˆê°€í•œ ì˜¤ë””ì˜¤ ê¸°ìˆ ì˜ ì´ˆê²©ì°¨ë¥¼ êµ¬í˜„í•´ë‚´ê³  ìˆìŠµë‹ˆë‹¤.</p>
            <p className="speaker"> ìˆ˜í¼í†¤ (TBA) <a className="link-badge" href='https://www.supertone.ai/ko'>Homepage</a></p>
            <p>ìˆ˜í¼í†¤ì˜ ìŠ¬ë¡œê±´ì¸ "Beyond the Voice"ì—ëŠ” ë‹¨ìˆœíˆ ìŒì„±ì„ ëª¨ë°©í•˜ëŠ” ê²ƒ ì´ìƒìœ¼ë¡œ ì„ êµ¬ì ì¸ AI ë³´ì´ìŠ¤ë¥¼ í–¥í•œ ìˆ˜í¼í†¤ì˜ í—Œì‹ ì´ ë‹´ê²¨ ìˆìŠµë‹ˆë‹¤. ìˆ˜í¼í†¤ì˜ ìŒì„± AIëŠ” ëª©ì†Œë¦¬ë¥¼ ì´í•´í•˜ê³ , ê³µëª…í•˜ë©°, ìŒì„±ì— í˜ì„ ë¶€ì—¬í•©ë‹ˆë‹¤. ìš°ë¦¬ì˜ ê¸°ìˆ ì€ ì»¤ë®¤ë‹ˆì¼€ì´ì…˜ì„ í˜ì‹ í•˜ê³  ë‹¤ì–‘í•œ ë¶„ì•¼ì— ì •ë°€í•¨ê³¼ ê°ì„±ì  ê¹Šì´ë¥¼ ë”í•©ë‹ˆë‹¤.</p>
            <p className="speaker"> ì— í”¼ì—ì´ì§€ (TBA) <a className="link-badge" href='https://www.mpaghq.com/'>Homepage</a></p>
            <p>MPAGëŠ” ëª¨ë‘ê°€ ìŒì•…ì„ ì¦ê¸¸ ìˆ˜ ìˆëŠ” â€‹ì§€ì†ê°€ëŠ¥í•œ í”Œë«í¼ì„ ë§Œë“­ë‹ˆë‹¤. â€‹ìŒì•…ì— ê¸°ìˆ ì„ ë”í•´ ìƒˆë¡œìš´ íë¦„ì„ ë§Œë“­ë‹ˆë‹¤. ê¸€ë¡œë²Œ ìŒì•… ìƒíƒœê³„ì—ì„œ ìš°ë¦¬ëŠ” ëŠì„ì—†ì´ ë„ì „í•˜ê³  ì—°êµ¬í•©ë‹ˆë‹¤. â€‹ì „ì„¸ê³„ ì´ìš©ìë“¤ì—ê²Œ ìµœê³ ì˜ ê²½í—˜ì„ ì œê³µí•˜ê³ , ìµœì‹  ê¸°ìˆ ë¡œ ë†€ë¼ìš´ ì„±ê³¼ë¥¼ ë§Œë“¤ê³  ìˆìŠµë‹ˆë‹¤.</p>
            <p className="speaker"> í¬ìë©ìŠ¤ (TBA) <a className="link-badge" href='https://www.pozalabs.com/'>Homepage</a></p>
            <p>í¬ìë©ìŠ¤ëŠ” ì¸ê³µì§€ëŠ¥(AI) ìŒì•… ìŠ¤íƒ€íŠ¸ì—…ì…ë‹ˆë‹¤. í¬ìë©ìŠ¤ëŠ” ê¸°ì¡´ì˜ ê³¡ì„ í•™ìŠµí•˜ì§€ ì•Šê³ , ë…ìì ì¸ ìŒì› ë°ì´í„°ë¥¼ ë§Œë“¤ì–´ ì¸ê³µì§€ëŠ¥ì— í•™ìŠµì‹œì¼œ ìŒì›ì„ ìƒì„±í•˜ê³  ìˆìœ¼ë©° "ëˆ„êµ¬ë‚˜ ì‰½ê²Œ ìì‹ ë§Œì˜ ìŒì•…ì„ ë§Œë“¤ê³  ì†Œìœ í•  ìˆ˜ ìˆëŠ” ì„¸ìƒì„ ë§Œë“ ë‹¤"ë¼ëŠ” ë¹„ì „ì„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤.</p>
          </>
        )}>
          <div className="time-badge">12:30</div>
          <h3>ì—°êµ¬ì‹¤ ë° ê¸°ì—… ì†Œê°œ ì„¸ì…˜ #1</h3>
          <div className="schedule-brief">4ê°œ ì—°êµ¬ì‹¤, 4ê°œ ê¸°ì—… ì†Œê°œ</div>
        </div>

        <div className="schedule-card" onClick={() => openModal('í•™ìƒ í¬ìŠ¤í„° ë°œí‘œ ì„¸ì…˜ #1', '13:30 - 15:00', 
          <>
            <p className="speaker">ê¹€ì„±ì˜ êµìˆ˜ (AIRIS Lab, KAIST)</p>
            <PosterList professor="AIRIS Lab, KAIST" posters={[
              "Pooseung KOH (ê³ ë¶€ìŠ¹): Augmented Reality Auditory Training for Selective Auditory Attention Enhancement",
              "Kangeun LEE (ì´ê°•ì€): Immersive Autotmatic Audio Panning System for Music Production",
              "Kyung Taek OH (ì˜¤ê²½íƒ): Aural Heritage: 6DoF Reconstruction of Cultural Heritage Sites",
              "Yideun PARK (ë°•ì´ë“ ): Quantitative and Qualitative Quotients in Music Source Separation: A Cross-Genre Analysis of Acoustic and Perceptual Performance with Emphasis on K-pop"
            ]} />
            <p className="speaker">ì´êµêµ¬ êµìˆ˜ (MARG, ì„œìš¸ëŒ€)</p>
            <PosterList professor="MARG, ì„œìš¸ëŒ€" posters={[
              "ì´ì„±í˜¸: Reverse Engineering of Music Mixing Graphs with Differentiable Processors and Iterative Pruning",
              "ì´ì„±í˜¸: Differentiable Acoustic Radiance Transfer",
              "ì±„ìœ¤ê¸°: Song Form-aware Full-Song Text-to-Lyrics Generation with Multi-Level Granularity Syllable Count Control",
              "ì •í•´ì„ : Optimizing Music Captioning with Reinforcement Learning and Retrieval-Augmented Methods",
              "ì •í•´ì„ : Exploring the Speech-to-Song Illusion: A Comparative Study of Standard Korean and Dialects",
              "ì‹ ì€ì‹: Synthetic Dataset Generation for String Ensemble Separation",
              "ì‹ ì›ì² : SynthRL: Cross-domain Synthesizer Sound Matching via Reinforcement Learning",
              "ê¹€í•˜ìœ¤: Expressive Singing Voice Synthesis",
              "í™©ì„ íƒœ: DOSE : Drum One-Shot Extraction from Music Mixture",
              "ì´í•˜ì¤€: Interpolative Timbre Transfer Across Domains",
              "ê¹€ìˆ˜ë¹ˆ: ERP responses of Interval Judgment in the Tritone Paradox",
              "ê¹€ì˜ˆì§„: Drum Generation with Latent Diffusion Models",
              "í•œë™ì—½: Hierarchical Music Transformer for MultiTrack Symbolic Music Generation",
              "í™ìœ ë‹ˆìŠ¤: Revisiting Mismatch Negativity: Additive Neural Processes Across Attention",
              "í™©ìƒˆì—°: The Analysis of Emotional Alignment between the Melody and Lyrics Extracted from Music"
            ]} />
            <p className="speaker">ì´ì„ì§„ êµìˆ˜ (AAA Lab, ê²½ë¶ëŒ€)</p>
            <PosterList professor="AAA Lab, ê²½ë¶ëŒ€" posters={["ì´ëŒ€í˜¸: Enhancement of Automatic Music Transcription Model with Number of Activated Pitch Information"]} />
            <p className="speaker">ì •ë‹¤ìƒ˜ êµìˆ˜ (Maler Lab, ì„œê°•ëŒ€)</p>
            <PosterList professor="Maler Lab, ì„œê°•ëŒ€" posters={[
              "ë°•í•œë‚˜(Hannah Park): GAON: Generative AI Offers Notes for your music",
              "ê¹€ëŒ€ì›…: ViolinDiff: Enhancing Expressive Violin Synthesis with Pitch Bend Conditioning",
              "ì •ì¢…ë¯¼: Unified Music Translation between Score Images, MusicXML, MIDI and Audio",
              "ì´ë‹¤ì†”: Understanding era gap between the US and Korean music charts using music CNN"
            ]} />
            <p className="speaker">ì´ì„±ì£¼ êµìˆ˜ (NMSL, KAIST)</p>
            <PosterList professor="NMSL, KAIST" posters={[
              "ê¹€ì˜ˆì› (Yewon Kim): Amuse: Human-AI Collaborative Songwriting with Multimodal Inspirations"
            ]} />
          </>
        )}>
          <div className="time-badge">13:30</div>
          <h3>í•™ìƒ í¬ìŠ¤í„° ë°œí‘œ ì„¸ì…˜ #1</h3>
          <div className="schedule-brief">ì´ 25ê°œ í¬ìŠ¤í„° ë°œí‘œ</div>
        </div>

        <div className="schedule-card break" onClick={() => openModal('íœ´ì‹', '15:00 - 15:15', 
          <>
            <p>Coffee Break</p>
          </>
        )}>
          <div className="time-badge">15:00</div>
          <h3>íœ´ì‹</h3>
          <div className="schedule-brief">Coffee Break</div>
        </div>

        <div className="schedule-card" onClick={() => openModal('ì—°êµ¬ì‹¤ ë° ê¸°ì—… ì†Œê°œ ì„¸ì…˜ #2', '15:15 - 16:15', 
          <>
            <p className="speaker">MuBL, KAIST (ì´ê²½ë©´ êµìˆ˜) <a className="link-badge" href='https://www.mubl.kaist.ac.kr/'>Homepage</a></p>
            <p>The place that investigates music and human, we are the Music and Brain Research Lab. We explore various aspects of humans who have evolved to enjoy music through interdisciplinary studies in musicology, brain science, and cognitive science.</p>
            <p className="speaker">SAPL, GIST (ì‹ ì¢…ì› êµìˆ˜) <a className="link-badge" href='https://www.youtube.com/watch?v=ipWoDTcGbMY&ab_channel=GISTAI%EB%8C%80%ED%95%99%EC%9B%90'>Homepage</a></p>
            <p>MSPLì—ì„œëŠ” ìŒì„±í†µì‹ , ì˜¤ë””ì˜¤, ìŒì„±, ê·¸ë¦¬ê³  ë©€í‹° ëª¨ë‹¬ ì‹ í˜¸ì²˜ë¦¬ì— ì¤‘ì ì„ ë‘” ë©€í‹°ë¯¸ë””ì–´ ë¶„ì•¼ì˜ ë‹¤ì–‘í•œ ì£¼ì œë¥¼ ë‹¤ë£¹ë‹ˆë‹¤. ì‹ í˜¸ íŠ¹ì„± ë° ì¸ì§€ ëª¨ë¸ì„ ê¸°ë°˜ìœ¼ë¡œí•œ ì‹ í˜¸ì²˜ë¦¬ì™€ ì‹ í˜¸ ëª¨ë¸ ë° ë°ì´í„°ë² ì´ìŠ¤ ì •ë³´ë¥¼ í™œìš©í•œ í†µê³„ì  ì ‘ê·¼ ë°©ì‹ì„ ì—°êµ¬í•´ì™”ê³ , ìµœê·¼ì—ëŠ” ë¨¸ì‹  ëŸ¬ë‹ì— ê¸°ë°˜í•œ ìŒì„±í–¥ìƒ, ê°ì •ì¸ì‹, ìŒì›ë¶„ë¦¬ ë“±ì˜ ì—°êµ¬ë¥¼ í•˜ê³  ìˆìŠµë‹ˆë‹¤.</p>
            <p className="speaker">MEMI Lab, GIST (ì•ˆì°½ìš± êµìˆ˜) <a className="link-badge" href='https://sites.google.com/view/gist-memi/'>Homepage</a></p>
            <p>"MEMI" conducts in-depth and wide-ranging research from source theory on evolutionary machine intelligence such as evolutionary computing, cluster intelligence, and deep learning to application technology with the aim of "realization of optimal solution of various real world problems". The purpose of this study is to develop new academic paradigm and to improve human welfare.</p>
            <p className="speaker">MAC Lab, KAIST (ë‚¨ì£¼í•œ êµìˆ˜) <a className="link-badge" href='https://mac.kaist.ac.kr/'>Homepage</a></p>
            <p>The Music and Audio Computing Lab (MACLab) is a music research group in the Graduate School of Culture Technology at KAIST. Our mission is to improve the way people enjoy, play, and create music through technology. We are particularly interested in "Music AI" that can understand music, represent its meanings in a human-friendly manner, and generate new musical content to assist human creativity.</p>
            <p className="speaker"> í¬ë¦¬ì—ì´í‹°ë¸Œë§ˆì¸ë“œ (ì•ˆì°½ìš± êµìˆ˜) <a className="link-badge" href='https://creativemind.ai'>Homepage</a></p>
            <p>ì „ë¬¸ ì‘ê³¡ê°€ì™€ ì‘ê³¡ì— ì§„ì‹¬ì¸ í¬ë¦¬ì—ì´í‹°ë¸Œë§ˆì¸ë“œê°€ ë§Œë“­ë‹ˆë‹¤. í¬ë¦¬ì—ì´í‹°ë¸Œë§ˆì¸ë“œëŠ” ì „ë¬¸ ì‘ê³¡ê°€ ë¿ ì•„ë‹ˆë¼ ì—”ì§€ë‹ˆì–´ê¹Œì§€ë„ ì‘ê³¡ì— ì§„ì‹¬ì…ë‹ˆë‹¤. ë§ì€ ì˜ˆìˆ ì¸ë“¤ì˜ ìƒˆë¡œìš´ ì°½ì‘ì„ ë•ê¸° ìœ„í—¤ MUSIA ì—”ì§„ì„ ê°œë°œí•˜ëŠ” ê²ƒì— í˜ì”ë‹ˆë‹¤. ë” ë§ì€ ì‚¬ëŒë“¤ì´ MUSIAë¥¼ í†µí•´ ìƒˆë¡œìš´ ì‘ê³¡ ì•„ì´ë””ì–´ë¥¼ ì–»ì–´ ë” í¸í•˜ê³  ë¹ ë¥´ê²Œ ì‘ê³¡ì„ í•  ìˆ˜ ìˆë„ë¡ ë…¸ë ¥í•©ë‹ˆë‹¤. MUSIA ì—”ì§„ì€ ê¸°ì¡´ì˜ ìŒì•… ë°ì´í„°ë¥¼ í•™ìŠµí•˜ì§€ ì•Šê³ , ìŒì•…ì˜ ê¸°ë³¸ ì›ë¦¬ í™”ì„±í•™ì„ í•™ìŠµí•˜ì—¬ ì¸ê°„ê³¼ ê°™ì€ ë°©ì‹ìœ¼ë¡œ ìŒì•…ì„ ì‘ê³¡í•©ë‹ˆë‹¤. MUSIAì™€ í•¨ê»˜í•˜ë©´ ì €ì‘ê¶Œì— ë°©í•´ë°›ì§€ ì•ŠëŠ” ì „í˜€ ìƒˆë¡œìš´ ë°©ì‹ì˜ ì‘ê³¡ ì•„ì´ë””ì–´ë¥¼ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.</p>
            <p className="speaker">MixAudio (TBA) <a className="link-badge" href='https://mix.audio/home'>Homepage</a></p>
            <p>We research advanced AI technology to present the most creative solutions in the music industry. With music and sound, every scene converts content, and everyone becomes the creator. Neutune provides audio assets and AI technology to help creators better tune their musical expressions.</p>
            <p className="speaker">SiriusXM/Pandora (ê¹€ì¬í›ˆ ë°•ì‚¬) <a className="link-badge" href='https://www.siriusxm.com/pandora'>Homepage</a></p>
            <p>SiriusXM is the leading audio entertainment company in North America with a portfolio of audio businesses including its flagship subscription entertainment service SiriusXM; the ad-supported and premium music streaming services of Pandora; an expansive podcast network; and a suite of business and advertising solutions. Reaching a combined monthly audience of approximately 150 million listeners, SiriusXM offers a broad range of content for listeners everywhere they tune in with a diverse mix of live, on-demand, and curated programming across music, talk, news, and sports.</p>
            <p className="speaker">Suno (ì›ìƒí¬ ë°•ì‚¬) <a className="link-badge" href='https://suno.com/'>Homepage</a></p>
            <p>Suno is building a future where anyone can make great music. Whether you're a shower singer or a charting artist, we break barriers between you and the song you dream of making. No instrument needed, just imagination. From your mind to music.</p>
          </>
        )}>
          <div className="time-badge">15:15</div>
          <h3>ì—°êµ¬ì‹¤ ë° ê¸°ì—… ì†Œê°œ ì„¸ì…˜ #2</h3>
          <div className="schedule-brief">4ê°œ ì—°êµ¬ì‹¤, 4ê°œ ê¸°ì—… ì†Œê°œ</div>
        </div>

        <div className="schedule-card" onClick={() => openModal('í•™ìƒ í¬ìŠ¤í„° ë°œí‘œ ì„¸ì…˜ #2', '16:15 - 17:45', 
          <>
            <p className="speaker">ì´ê²½ë©´ êµìˆ˜ (MuBL, KAIST)</p>
            <PosterList professor="MuBL, KAIST" posters={[
              "ê¹€í˜„ì¬: The Emergence of Musicality in Deep Auditory Models: Learning from Non-Musical Natural Sounds",
              "ì˜¤ì€ì§€: A Real-Time EEG Synchrony System for Visualizing Shared Musical Pleasure",
              "ìµœìœ ì§„: Avatar-Based Audience Experiences in VR Music Concerts: Flow and Social Bonding",
              "ê¹€íš¨ì§„: Exploring a New Modality for Music Emotion: Emoji-Based Real-Time Chat Data",
              "ì†¡íƒœì¸: Review : Research on the Musical Emotions of Cochlear Implant Users"
            ]} />
            <p className="speaker">ì‹ ì¢…ì› êµìˆ˜ (SAPL, GIST)</p>
            <PosterList professor="SAPL, GIST" posters={["ì†ì£¼í˜œ: Band Splitting-based Online Music Source Separation"]} />
            <p className="speaker">ì•ˆì°½ìš± êµìˆ˜ (MEMI Lab, GIST)</p>
            <PosterList professor="MEMI Lab, GIST" posters={["Taehyeon Kim (ê¹€íƒœí˜„): Multi-Task Learning based Temporal Pattern Matching Network for Guitar Tablature Transcription"]} />
            <p className="speaker">ë‚¨ì£¼í•œ êµìˆ˜ (MAC Lab, KAIST)</p>
            <PosterList professor="MAC Lab, KAIST" posters={["ê¹€í˜„ìˆ˜ (Hounsu Kim): D3RM: A Discrete Diffusion Refinement Model for Piano Transcription", "ìµœì€ì§„ (Eunjin Choi): D3PIA: A Discrete Denoising Diffusion Model for Piano Accompaniment Generation", "ìµœì€ì§„ (Eunjin Choi): On the de-duplication of the Lakh MIDI dataset", "ë°©í•˜ì—° (Hayeon Bang): PianoBind: A Multimodal Joint Embedding Model for Pop-piano Music", "ë„ìŠ¹í—Œ (SeungHeon Doh): Connecting Large Langauge Models and Music", "ì´ì¤€ì› (Junwon Lee): Controllable Foley Sound Generation from Multimodal Inputs", "ê¹€ê¸°ë½ (Kirak Kim): Pianist Hand Motion Generation using Diffusion Models", "ê¶ŒëŒ€ìš© (Daeyong Kwon): MusT-RAG: Musical Text Question Answering with Retrieval Augmented Generation", "ê¹€ë‹¤ë¹ˆ (Dabin Kim): Timbre Transfer for Monophonic Musical Instruments with Text-to-Audio Diffusion Models", "ë°°ì¤€í˜• (Joonhyung Bae): A Web-Based Dashboard for Integrated Analysis and Visualization of Multimodal Piano Performance Data", "í•œë‹¨ë¹„ë‚´ë¦° (Danbinaerin Han): Audio-based repeated melodic phrases in Korean folk singing recordings", "ë‹¤ë‹ˆì—˜ ë¹ˆ ìŠˆë¯¸ë“œ (Daniel BÄ«n Schmid): DIPO: Diffusion Inference-Time Partial Optimization for Structure Preserving Content Editing", "ë°•ìƒˆë³„ (Saebyul Park): Quantitative Analysis of Melodic Similarity in Music Copyright Infringement Cases"]} />
          </>
        )}>
          <div className="time-badge">16:15</div>
          <h3>í•™ìƒ í¬ìŠ¤í„° ë°œí‘œ ì„¸ì…˜ #2</h3>
          <div className="schedule-brief">ì´ 20ê°œ í¬ìŠ¤í„° ë°œí‘œ</div>
        </div>

        <div className="schedule-card" onClick={() => openModal('ííšŒ ë° ê¸°ë… ì´¬ì˜', '17:45 - 18:00', 
          <>
            <p>ì‹¬í¬ì§€ì—„ ë‹¨ì²´ ì‚¬ì§„ ì´¬ì˜ì´ ìˆì„ ì˜ˆì •ì…ë‹ˆë‹¤.</p>
          </>
        )}>
          <div className="time-badge">17:45</div>
          <h3>ííšŒ ë° ê¸°ë… ì´¬ì˜</h3>
          <div className="schedule-brief">ë‹¨ì²´ ì‚¬ì§„ ì´¬ì˜</div>
        </div>

        <div className="schedule-card" onClick={() => openModal('ë§Œì°¬, ë„¤íŠ¸ì›Œí‚¹ í–‰ì‚¬', '18:00 - 20:00', 
          <>
            <p>ì €ë… ì‹ì‚¬ì™€ í•¨ê»˜í•˜ëŠ” ë„¤íŠ¸ì›Œí‚¹ ì‹œê°„ì…ë‹ˆë‹¤.</p>
          </>
        )}>
          <div className="time-badge">18:00</div>
          <h3>ë§Œì°¬, ë„¤íŠ¸ì›Œí‚¹ í–‰ì‚¬</h3>
          <div className="schedule-brief">ì €ë… ë§Œì°¬ ë° êµë¥˜</div>
        </div>
      </div>

      {/* Modal Component */}
      {modalOpen && (
        <div className="modal-overlay" onClick={closeModal}>
          <div className="modal-content" onClick={e => e.stopPropagation()}>
            <div className="modal-header">
              <h2>{modalContent.title}</h2>
              <div className="modal-time">{modalContent.time}</div>
              <button className="close-button" onClick={closeModal}>Ã—</button>
            </div>
            <div className="modal-body">
              {modalContent.content}
            </div>
          </div>
        </div>
      )}
    </section>
  );
}

export default Schedule; 